% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/xgb_autotune.R
\name{xgb_autotune}
\alias{xgb_autotune}
\title{xgb_autotune}
\usage{
xgb_autotune(dtrain, x, y, w = NULL, base_margin = NULL, xgbParams, nrounds,
  early_stopping_rounds, nfold, folds = NULL, verbose = TRUE, seed = 1921,
  maximize = FALSE, bounds, init_points, n_iter, init_grid_dt = NULL, ...)
}
\arguments{
\item{dtrain}{The training data for the optimisation}

\item{x}{A list that identifies the features}

\item{y}{A string that identifies the label}

\item{w}{A string that identifies the weight column. Defaults to NULL.}

\item{base_margin}{A string that identifies the base_margin (offset). Defaults to NULL}

\item{xgbParams}{A list of extra parameters to be passed to xgb.cv}

\item{nrounds}{the max number of iterations}

\item{early_stopping_rounds}{If set to an integer k, training with a validation set will stop if the performance doesn't improve for k rounds.}

\item{nfold}{the original dataset is randomly partitioned into nfold equal size subsamples.}

\item{folds}{list provides a possibility to use a list of pre-defined CV folds (each element must be a vector of test fold's indices).}

\item{verbose}{Whether or not to print progress.}

\item{seed}{Random State}

\item{maximize}{Should the loss function be maximized or not?}

\item{bounds}{A named list of lower and upper bounds for each hyperparameter. Please use "L" suffix to indicate integer hyperparameter.}

\item{init_points}{Number of randomly chosen points to sample the target function before Bayesian Optimization fitting the Gaussian Process.}

\item{n_iter}{Total number of times the Bayesian Optimization is to repeated.}

\item{init_grid_dt}{User specified points to sample the target function, should be a data.frame or data.table with identical column names as bounds.}

\item{...}{Additional args to be passed to BayesOptim}
}
\description{
Run an automatic bayesian optimisation of xgboost
}
\examples{

}
\keyword{autotune}
\keyword{xgboost}
